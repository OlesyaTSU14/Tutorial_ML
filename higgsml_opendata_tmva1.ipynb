{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile,TTree,gStyle,TMVA,TCut,TH1F\n",
    "import array\n",
    "gStyle.SetOptStat(1111111)\n",
    "\n",
    "import random,string,math,csv\n",
    "\n",
    "debug=False\n",
    "debug=True # if print some debugging printout\n",
    "\n",
    "docsvtoroot=False    \n",
    "dotraining=False     \n",
    "doevaluate=False     \n",
    "dothreshold=False    \n",
    "dosubmission=False   # \n",
    "\n",
    "# uncomment the one you do not want\n",
    "docsvtoroot=True # read csv file, convert into root file\n",
    "dotraining=True # train on training file, save training in xml file\n",
    "doevaluate=True # apply training on training and test files, output new root files with score variable\n",
    "dothreshold=True # compute optimal threshold from score training file\n",
    "dosubmission=True # generate csv file for kaggle submission, from optimal threshold and test score file\n",
    "\n",
    "filenamecsv=\"atlas-higgs-challenge-2014-v2.csv\"\n",
    "treename=\"htautau\"\n",
    "picklename=\"threshold.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python\n",
    "\n",
    "from ROOT import TFile,TTree,gStyle,TMVA,TCut,TH1F\n",
    "import array\n",
    "gStyle.SetOptStat(1111111)\n",
    "\n",
    "#%matplotlib inline\n",
    "from array import array\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "import random,string,math,csv\n",
    "\n",
    "debug=False\n",
    "debug=True # if print some debugging printout\n",
    "\n",
    "docsvtoroot=False    \n",
    "dotraining=False     \n",
    "doevaluate=False     \n",
    "dothreshold=False    \n",
    "dosubmission=False   # \n",
    "\n",
    "# uncomment the one you do not want\n",
    "docsvtoroot=True # read csv file, convert into root file\n",
    "dotraining=True # train on training file, save training in xml file\n",
    "doevaluate=True # apply training on training and test files, output new root files with score variable\n",
    "dothreshold=True # compute optimal threshold from score training file\n",
    "dosubmission=True # generate csv file for kaggle submission, from optimal threshold and test score file\n",
    "\n",
    "filenamecsv=\"atlas-higgs-challenge-2014-v2.csv\"\n",
    "treename=\"htautau\"\n",
    "picklename=\"threshold.p\"\n",
    "\n",
    "\n",
    "# convert the cvs file into a root file\n",
    "# bit by bit conversion, except the \"Label\" which is converted into an int 1 for \"s\" 0 for \"b\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if docsvtoroot:\n",
    "    print \" Step #1 : read csv file, convert into root file \"\n",
    "    pathtofile=\"\"\n",
    "\n",
    "    fullfilename=pathtofile+filenamecsv\n",
    "    print \"reading \",fullfilename\n",
    "    allentries = list(csv.reader(open(fullfilename,\"rb\"), delimiter=','))\n",
    "\n",
    "    # first line is the list of variables, put it aside\n",
    "    header        = allentries.pop(0)\n",
    "    if debug:\n",
    "        print header\n",
    "\n",
    "    output_name  = filenamecsv+'.root'\n",
    "    output_file  = TFile(output_name, 'recreate')\n",
    "\n",
    "    # create ttree\n",
    "    tree = TTree(treename, treename)\n",
    "\n",
    "    # mapping between name and field\n",
    "    maps={}\n",
    "\n",
    "    # mapping of variable name with type\n",
    "    typemap={}\n",
    "\n",
    "    # by default all variables are float\n",
    "    for var in header:\n",
    "        typemap [ var ] = \"float\"\n",
    "\n",
    "    # create the new branches if any\n",
    "    newvars= []\n",
    "    # newvars= [\"MyGreatNewVar\"] \n",
    "    for var in newvars:\n",
    "        maps [ var ] = array.array('f',[0.0])\n",
    "        tree.Branch(var , maps[var],  '% s/F' % (var))  # assume all new vars are float\n",
    "\n",
    "        \n",
    "    # deal with some exception\n",
    "    typemap[\"EventId\"]=\"int\" \n",
    "    typemap[\"PRI_jet_num\"]=\"int\"\n",
    "    typemap[\"Label\"]=\"int\" # s/b will be converted to 1/0\n",
    "    typemap[\"KaggleSet\"]=\"int\" # t b v u will be converted in 0 10 11 100\n",
    "\n",
    "\n",
    "    if len(typemap)!=len(header): # we should not have added any variable\n",
    "        print \"typemap :\", len(typemap), \"header : \", len(header) # we should not have added any variable\n",
    "        exit() # should not happen\n",
    "\n",
    "\n",
    "    # create the tree branches\n",
    "    for var in header:\n",
    "        type=typemap[var]\n",
    "        if type==\"float\":\n",
    "            maps [ var ] = array.array('f',[0.0])\n",
    "            tree.Branch(var , maps[var],  '% s/F' % (var))\n",
    "        elif type==\"int\":\n",
    "            maps [ var ] = array.array('i',[0])\n",
    "            tree.Branch(var , maps[var],  '% s/I' % (var))\n",
    "\n",
    "    if debug:\n",
    "        tree.GetListOfBranches().Print()    \n",
    "\n",
    "    # now fill the tree    \n",
    "    ientry=0\n",
    "    nvar=len(header)\n",
    "    for entry in allentries:\n",
    "        for ivar in range(nvar):\n",
    "            varval=entry[ivar]\n",
    "            varname=header[ivar]\n",
    "            type=typemap[varname]\n",
    "\n",
    "            # convert the character variables into int\n",
    "            if varname==\"Label\":\n",
    "                if varval==\"s\":\n",
    "                    varval=1\n",
    "                elif varval==\"b\":\n",
    "                    varval=0\n",
    "                else:\n",
    "                    varval=-999\n",
    "            if varname==\"KaggleSet\":\n",
    "                if varval==\"t\":\n",
    "                    varval=0\n",
    "                elif varval==\"b\":\n",
    "                    varval=10\n",
    "                elif varval==\"v\":\n",
    "                    varval=11\n",
    "                elif varval==\"u\":\n",
    "                    varval=100\n",
    "                else:\n",
    "                    varval=-999\n",
    "\n",
    "            if type==\"float\":        \n",
    "                maps [ varname ] [0]= float(varval)\n",
    "            elif type==\"int\":\n",
    "                maps [ varname ] [0]= int(varval)\n",
    "\n",
    "            # now fill the new variables if any\n",
    "            if len(newvars)!=0:\n",
    "                maps[\"MyGreatNewVar\"][0]=maps[\"PRI_lep_phi\"][0]+1.213141 # some calculation\n",
    "\n",
    "\n",
    "                \n",
    "        tree.Fill()\n",
    "        # if ientry>10:\n",
    "        #    break\n",
    "        ientry+=1\n",
    "        if ientry % 10000 == 1:\n",
    "            print \"processing event \",ientry\n",
    "\n",
    "    print ientry, \" entries successfully written \"\n",
    "    # Save and close the output file (required for data to be written)    \n",
    "    output_file.Write()\n",
    "    output_file.Close()\n",
    "    # cleanup this big object\n",
    "    del allentries\n",
    "\n",
    "# train from training.csv.root\n",
    "# output training in xml\n",
    "# training is the most simple BDT in TMVA\n",
    "# this is deliberate to leave room for improvements within TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dotraining:\n",
    "    print \" Step #2 : train on training file, save training in xml file\"\n",
    "    output_directory = 'higgsml_output'\n",
    "    traintree_name = treename\n",
    "    trainfilename=filenamecsv+\".root\" \n",
    "\n",
    "\n",
    "    trainfile = TFile.Open(trainfilename,\"read\")\n",
    "    traintree = trainfile.Get(traintree_name)\n",
    "    \n",
    "    TMVA.Tools.Instance()\n",
    "    \n",
    "    \n",
    "    # create the tmva output file, which will be full of details about the training\n",
    "    fout = TFile(\"tmvatest.root\",\"RECREATE\")\n",
    "\n",
    "\n",
    "    # use the default factory\n",
    "    factory = TMVA.Factory(\"TMVAClassification\", fout)                                \n",
    "\n",
    "\n",
    "\n",
    "    # build the list of variables\n",
    "    al=traintree.GetListOfBranches()\n",
    "    varlist=[]\n",
    "    for i in range(al.GetEntries()):\n",
    "        varlist+=[al[i].GetName()]\n",
    "\n",
    "        \n",
    "    if debug:\n",
    "        print \"all variables of \",trainfile, \" \", varlist\n",
    "        print \"now stripping EventId Weight and Label \"\n",
    "\n",
    "    # these variables should not be used for training\n",
    "    mva_input_list=[e for e in varlist if not e in ['EventId','Weight','Label','KaggleSet','KaggleWeight']] \n",
    "    if debug:\n",
    "        print \"all input variables to BDT \", mva_input_list\n",
    "    if len(mva_input_list)!=len(varlist)-5:\n",
    "        raise Exception #  Something not understood in building mva_input_list\n",
    "\n",
    "    # only use kaggle training set (arbitrary) KaggleSet==0    \n",
    "    # signal selection    \n",
    "    signalCut = TCut( \"Label==1 && KaggleSet==0\" ) \n",
    "    # background selection\n",
    "    backgrCut = TCut( \"Label==0 && KaggleSet==0\" ) \n",
    "\n",
    "    # only one input tree\n",
    "    factory.SetInputTrees(traintree, signalCut, backgrCut );\n",
    "    \n",
    "\n",
    "    n=0\n",
    "    # declare all variables (all float except one int )\n",
    "    for var in mva_input_list:\n",
    "        if var==\"PRI_jet_num\":\n",
    "            factory.AddVariable( var, var, \"units\", 'I' );\n",
    "        else:\n",
    "            factory.AddVariable( var, var, \"units\", 'D' );\n",
    "        n +=1\n",
    "\n",
    "    # specify weight. Can use KaggleWeight when using kaggle training set\n",
    "\n",
    "    # if want to use the complete data set, then one should use variable Weight\n",
    "\n",
    "    # if want to use another subset, then one should use variable\n",
    "    # for signal event (Label==1)     : Weight*(Sum_(i all signal) Weight_i)) /(Sum_(i subset signal) Weight_i))\n",
    "    # for background event (Label==0) : Weight*(Sum_(i all background) Weight_i)) /(Sum_(i subset background) Weight_i))\n",
    "    # (KaggleWeight was computed like that for Kaggle subsets)     \n",
    "    factory.SetWeightExpression(\"KaggleWeight\");\n",
    "    \n",
    "\n",
    " \n",
    "    # SplitMode=block so that the order is respected.\n",
    "    # usually TMVA split the input file in two, one for training, one for testing\n",
    "    # but we want to use the full training.csv file for training, since the testing will be done from test.csv\n",
    "    # TMVA way of doing this is to say we want just 1 signal and 1 background test event. ALl the others will be used for training\n",
    "    factory.PrepareTrainingAndTestTree( signalCut, backgrCut,\"nTrain_Signal=0:nTrain_Background=0:nTest_Signal=1:nTest_Background=1:SplitMode=Block\")\n",
    "    \n",
    "    # use BDT out of the box, no attempt to fine tune\n",
    "    method = factory.BookMethod(TMVA.Types.kBDT, \"BDT\")\n",
    "\n",
    "\n",
    "    # launch the training\n",
    "    factory.TrainAllMethods()\n",
    "    # factory.TestAllMethods()\n",
    "    # factory.EvaluateAllMethods()\n",
    "    \n",
    "    fout.Close()\n",
    "    \n",
    "\n",
    "\n",
    "# read in train and test file, apply scoring as described in weights directory\n",
    "# write out new root tree with additional score file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doevaluate:\n",
    "    print \" Step #3 : apply training on training and test files, output new root files with score variable in addition\"\n",
    "\n",
    "\n",
    "    inputfilename=filenamecsv+\".root\"\n",
    "    outputfilename=filenamecsv+\"_score.root\"\n",
    "\n",
    "\n",
    "    new_variables_list = [\n",
    "        'bdt',\n",
    "        ]\n",
    "\n",
    "\n",
    "    maps = {}\n",
    "\n",
    "    inputfile = TFile.Open(inputfilename,\"read\")\n",
    "    inputtree    = inputfile.Get(treename)\n",
    "\n",
    "    # get the list of input variables\n",
    "    al=inputtree.GetListOfBranches()\n",
    "    varlist=[]\n",
    "    for i in range(al.GetEntries()):\n",
    "        varlist+=[al[i].GetName()]\n",
    "    mva_input_list=[e for e in varlist if not e in ['EventId','Weight','Label','KaggleSet','KaggleWeight']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create array to map variables to values\n",
    "    for var in varlist:                \n",
    "        maps [var ] = array.array('f',[0.0]) # all float, including possible integer (otherwise the reader chokes)          \n",
    "\n",
    "    for var in new_variables_list: \n",
    "        if debug:\n",
    "            print \"adding \",var\n",
    "        maps [var ] = array.array('f',[0.0])\n",
    "\n",
    "\n",
    "    reader=TMVA.Reader()\n",
    "    # Add to the reader the variables to be used for bdt evaluation (only that one)\n",
    "    # Make sure variables are in the same order as for training\n",
    "\n",
    "    for var in mva_input_list:                \n",
    "        reader.AddVariable( var , maps[var]);            \n",
    "\n",
    "    reader.BookMVA(\"BDT\",\"weights/TMVAClassification_BDT.weights.xml\")\n",
    "\n",
    "\n",
    "    # Create the output file\n",
    "    print \"creating file \",outputfilename\n",
    "    outputfile = TFile.Open(\n",
    "        outputfilename, \n",
    "        'RECREATE'\n",
    "        )\n",
    "\n",
    "    # Clone the original chain (but don't copy any entries yet).\n",
    "    outputtree = inputtree.CloneTree(0)\n",
    "\n",
    "    # Create other derived branches in the new tree\n",
    "    for var in new_variables_list:\n",
    "            outputtree.Branch(var , maps[var],  '% s/F' % (var))\n",
    "\n",
    "    if debug:\n",
    "        print \"List of branches for output tree\"\n",
    "        outputtree.GetListOfBranches().Print()\n",
    "\n",
    "\n",
    "    # Loop over the original tree.\n",
    "    max_index = inputtree.GetEntries()\n",
    "    # max_index=min(max_index,10) ; # print \"DR hack max event\",max_index\n",
    "    for i in xrange(0, max_index):\n",
    "        if i % 10000==1:\n",
    "            print \" processing event \",i \n",
    "\n",
    "        # Now actually load the tree data.  This needs to be done before\n",
    "        # doing any manual selections or calculations.\n",
    "        inputtree.GetEntry(i)\n",
    "\n",
    "        # copy the input tree variables into the arrays (might be a better way)\n",
    "        # actually this is not be necessary for variables that are not used by BDT\n",
    "        for var in varlist:\n",
    "            exec ('maps[\"'+var+'\"][0]=inputtree.'+var)\n",
    "\n",
    "        # compute bdt scoree    \n",
    "        maps['bdt'][0]=reader.EvaluateMVA(\"BDT\")\n",
    "\n",
    "        # Add this entry in the new tree\n",
    "        outputtree.Fill()\n",
    "\n",
    "\n",
    "    # Save the output file (required for data to be written)\n",
    "    outputfile.Write()\n",
    "\n",
    "    # Close the output file\n",
    "    outputfile.Close()\n",
    "\n",
    "\n",
    "# compute a threshold for the best AMS\n",
    "# plot the different AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dothreshold:\n",
    "    print \" Step #4 : compute optimal threshold from score training file\"\n",
    "\n",
    "    def amssimple(s,b):\n",
    "        from math import sqrt\n",
    "        if b==0:\n",
    "            return 0\n",
    "        return s/sqrt(float(b))\n",
    "\n",
    "    def amsasimov(s,b):\n",
    "        from math import sqrt,log\n",
    "        if b==0:\n",
    "            return 0\n",
    "\n",
    "        return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
    "\n",
    "    def amsfinal(s,b):\n",
    "        return amsasimov(s,b+10.)\n",
    "\n",
    "    def printsig(s,b):\n",
    "        return \"ams simple=%.3f asimov=%.3f final=%.3f\" % (amssimple(s,b),amsasimov(s,b),amsfinal(s,b))\n",
    "\n",
    "    # function to return a vector of bin content of an histogram\n",
    "    def getbins ( h):\n",
    "        vals=[]\n",
    "        vales=[]\n",
    "        nbins=h.GetNbinsX()\n",
    "        # copy the bins. Note that bin 0 is the underflow, bin nbins+1 is overflow    \n",
    "        for i in range (0,nbins+2): \n",
    "            vals+=[h.GetBinContent(i)]\n",
    "            vales+=[h.GetBinError(i)]            \n",
    "        underflow=h.GetBinContent(0)\n",
    "        overflow=h.GetBinContent(nbins+1)        \n",
    "        if underflow>0:\n",
    "            print \" WARNING histo \", h.GetName(), \" underflow non zero :\",underflow,\" deliberate ?\"\n",
    "        if overflow>0:    \n",
    "            print \" WARNING histo \", h.GetName(), \" overflow non zero :\",overflow,\" deliberate ?\"            \n",
    "        return vals,vales    \n",
    "\n",
    "    \n",
    "    inputfilename= filenamecsv+\"_score.root\"\n",
    "    inputfile = TFile.Open(inputfilename,\"read\")\n",
    "    inputtree    = inputfile.Get(treename)\n",
    "\n",
    "    nbins=1000\n",
    "    scoremin=-1\n",
    "    scoremax=1\n",
    "    hsig=TH1F(\"hsig\",\"signal bdt score\",nbins,scoremin,scoremax)\n",
    "    hbkg=TH1F(\"hbkg\",\"background bdt score\",nbins,scoremin,scoremax)    \n",
    "    # accumulate sum of square weigths\n",
    "    hsig.Sumw2()\n",
    "    hbkg.Sumw2()    \n",
    "\n",
    "    \n",
    "    # fill histograms, using weight, only for training sample\n",
    "    inputtree.Project(\"hsig\",\"bdt\",\"KaggleWeight*(Label==1 && KaggleSet==0)\")\n",
    "    inputtree.Project(\"hbkg\",\"bdt\",\"KaggleWeight*(Label==0 && KaggleSet==0)\")    \n",
    "\n",
    "    # copy into vectors\n",
    "    vsig,ve=getbins(hsig)\n",
    "    vbkg,ve=getbins(hbkg)    \n",
    "\n",
    "    # prepare other vectors\n",
    "    vsigint=[0]*(nbins+2)\n",
    "    vbkgint=[0]*(nbins+2)\n",
    "    vamsasimov=[0]*(nbins+2)\n",
    "    vamsfinal=[0]*(nbins+2)\n",
    "    vamssimple=[0]*(nbins+2)    \n",
    "\n",
    "    # signal has higher values than background. So integrate histograms from the right\n",
    "    sumsig=0\n",
    "    sumbkg=0\n",
    "    for i in range (nbins+1,-1,-1):\n",
    "        sumsig+=vsig[i]\n",
    "        vsigint[i]=sumsig\n",
    "        sumbkg+=vbkg[i]\n",
    "        vbkgint[i]=sumbkg\n",
    "        vamsasimov[i]=amsasimov(sumsig,sumbkg)\n",
    "        vamssimple[i]=amssimple(sumsig,sumbkg)\n",
    "        vamsfinal[i]=amsfinal(sumsig,sumbkg)        \n",
    "\n",
    "    hsigint=TH1F(\"hsigint\",\"integrated signal vs bdt score\",nbins,scoremin,scoremax)\n",
    "    hbkgint=TH1F(\"hbkgint\",\"integrated bkg vs bdt score\",nbins,scoremin,scoremax)        \n",
    "    hamsfinal=TH1F(\"hamsfinal\",\"final ams vs bdt score\",nbins,scoremin,scoremax)\n",
    "    hamsasimov=TH1F(\"hamsasimov\",\"asimov ams vs bdt score\",nbins,scoremin,scoremax)\n",
    "    hamssimple=TH1F(\"hamssimple\",\"simple ams vs bdt score\",nbins,scoremin,scoremax)\n",
    "\n",
    "    # copy vectors into histogram\n",
    "    for i in range(nbins+2):\n",
    "        hsigint.SetBinContent(i,vsigint[i])\n",
    "        hbkgint.SetBinContent(i,vbkgint[i])        \n",
    "        hamsfinal.SetBinContent(i,vamsfinal[i])\n",
    "        hamssimple.SetBinContent(i,vamssimple[i])\n",
    "        hamsasimov.SetBinContent(i,vamsasimov[i])        \n",
    "\n",
    "    # one nice plot    \n",
    "    hamsfinal.SetLineColor(2)\n",
    "    hamssimple.SetLineColor(3)   \n",
    "    hamsfinal.Draw()\n",
    "    hamssimple.Draw(\"same\")\n",
    "    hamsasimov.Draw(\"same\")\n",
    "   \n",
    "    # determine the optimal value\n",
    "    # Note that we determine the optimal value on the same data as used for the training.\n",
    "    # There are many ways to do better\n",
    "    bestamsfinal=max(vamsfinal)\n",
    "    ibest=vamsfinal.index(bestamsfinal)\n",
    "    threshold=hamsfinal.GetBinLowEdge(ibest+1)\n",
    "    print \"Best amsfinal \",bestamsfinal,\" for threshold :\",threshold,\" ( ams simple =\",vamssimple[ibest],\", ams asimov=\",vamsasimov[ibest],\")\"\n",
    "\n",
    "    print \" Writing out threshold value \",threshold, \" in pickle file:\",picklename\n",
    "    import pickle\n",
    "    pickle.dump(threshold,open (picklename,\"wb\"))\n",
    "\n",
    "\n",
    "# create kaggle submission file (just for reference)\n",
    "# ranks all entries according to score\n",
    "# given a threshold, label entries \"s\" or \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dosubmission:\n",
    "    print \" Step #5 : generate csv file for kaggle submission, from optimal threshold and test score file\"\n",
    "    import pickle\n",
    "    threshold=pickle.load( open( picklename, \"rb\" ) )\n",
    "    print \" Reading out threshold value \",threshold, \" from pickle file:\",picklename    \n",
    "\n",
    "    inputfilename=filenamecsv+\"_score.root\"\n",
    "    submissionfilename=\"submission_tmva.csv\"\n",
    "\n",
    "    inputfile = TFile.Open(inputfilename,\"read\")\n",
    "    inputtree    = inputfile.Get(treename)\n",
    "\n",
    "\n",
    "    print  \"Loop once to load the EventId, bdt score pairs\"\n",
    "    max_index = inputtree.GetEntries()\n",
    "    testentries=[]\n",
    "    for i in xrange(0, max_index):\n",
    "        if i % 100000==1:\n",
    "            print \" processing event \",i \n",
    "        inputtree.GetEntry(i)\n",
    "        # only consider event from the public and private dataset\n",
    "        if inputtree.KaggleSet not in [10,11]:\n",
    "            continue\n",
    "        testentries+=[(inputtree.EventId,inputtree.bdt)]\n",
    "\n",
    "    # sort on the bdt,\n",
    "    print \"sort on the bdt score\"\n",
    "    testsorted=sorted(testentries,key=lambda entry: entry[1])\n",
    "\n",
    "    # then build a map key Id, value rank\n",
    "    testdict={}\n",
    "    r=1 # kaggle ask to start at 1\n",
    "    for e in testsorted:\n",
    "        if r % 100000==1:\n",
    "            print \" sorting event \",r \n",
    "\n",
    "        testdict[e[0]]=r\n",
    "        r+=1\n",
    "\n",
    "        \n",
    "    print \"size of dict\",len(testdict)\n",
    "    if len(testdict)!=len(testsorted):\n",
    "        print \" ERROR ! Size of input file : \",len(testsorted), \" size of map : \",len(testdict), \" Should be equal. Maybe identical values ? \" \n",
    "        raise Exception # should not happen\n",
    "\n",
    "\n",
    "    outputfile=open(submissionfilename,\"w\")\n",
    "    outputfile.write(\"EventId,RankOrder,Class\\n\")\n",
    "\n",
    "    print  \"Loop again to write the submission file\",submissionfilename\n",
    "    for i in xrange(0, max_index):\n",
    "        if i % 100000==1:\n",
    "            print \" processing event \",i \n",
    "                \n",
    "        inputtree.GetEntry(i)\n",
    "        if inputtree.KaggleSet not in [10,11]:\n",
    "            continue\n",
    "\n",
    "        rank=testdict[inputtree.EventId]\n",
    "        if rank>550000:\n",
    "            print \"large rank: \",rank, \" for event \",i, \" \",inputtree.EventId\n",
    "            exit() # should not happen\n",
    "        \n",
    "        # compute label \n",
    "        slabel=\"b\"\n",
    "        if inputtree.bdt>threshold:\n",
    "            slabel=\"s\"\n",
    "            \n",
    "        outputfile.write(str(inputtree.EventId)+\",\")\n",
    "        outputfile.write(str(rank)+\",\")\n",
    "        outputfile.write(slabel)            \n",
    "        outputfile.write(\"\\n\")\n",
    "\n",
    "    outputfile.close()\n",
    "    print \"All done!\"\n",
    "    # delete big objects\n",
    "    del testentries\n",
    "    del testdict\n",
    "    del testsorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
